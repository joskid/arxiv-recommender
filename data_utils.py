# Utility functions to load and pre-process data.

import numpy as np
import time
import os
import argparse
import glob
from collections import OrderedDict
from pprint import pprint

def load_vocab(fpath):
	"""
	Reads the VOCAB_FILE produced by the PTBTokenizer and returns all 
	(token, count) pairs in a numpy array.
	"""
	return np.genfromtxt(fpath, dtype=("U25, int"), comments=None, delimiter=" ")

def load_embeddings(fpath):
	"""
	Reads the SAVE_FILE produced by the GloVe model and returns the
	(token, vector) pairs in an ordered dictionary.
	"""
	start = time.time()
	print("Loading pre-trained GloVe word embeddings.")
	embeddings = OrderedDict()
	for line in open(fpath):
		splitLine = line.split(" ")
		token = splitLine[0]
		vector = np.array([float(value) for value in splitLine[1:]])
		embeddings[token] = vector
	print("Finished loading embeddings. Time taken: %.2f seconds."%(time.time()-start))
	return(embeddings)

def load_embeddings_array(fpath):
	"""
	Reads the SAVE_FILE produced by the GloVe model and returns the
	vocabulary and vectors in separate arrays. 
	"""
	start = time.time()
	print("Loading pre-trained GloVe word embeddings.")
	vocab = []
	embeddings = []
	for line in open(fpath):
		splitLine = line.split(" ")
		vocab.append(splitLine[0])
		embeddings.append([float(value) for value in splitLine[1:]])
	print("Finished loading embeddings. Time taken: %.2f seconds."%(time.time()-start))
	return(np.array(vocab), np.array(embeddings))

def load_labels(topics_file, assignments_file):
	"""
	Reads the lda_topics and lda_assignments files generated by lda.py.
	Returns the ground truth labels for each abstract, along with the topics
	"""
	# read in LDA topics
	topics = []
	with open(topics_file) as f:
		for line in f.read().split("\n"):
			topics.append(line)
	# remove last element which is empty
	topics.pop()	
	# read in assignments
	labels = np.genfromtxt(assignments_file, dtype=int, delimiter="\n")
	print("Finished loading LDA topic labels.")
	return (topics, labels)

def load_abstracts(abs_dir_tok):
	"""
	Reads in the tokenized abstracts, stored in individual text files, and returns
	the file names and tokenized abstracts in two separate lists.
	"""
	fnames = []
	abstracts = []
	for file in glob.glob(abs_dir_tok + "/*"):
		with open(file, "r") as f:
			abstracts.append(f.read())
			fnames.append(os.path.basename(f.name))
	print("Finished loading tokenized abstracts.")
	return(fnames, abstracts)

def pad_abstracts(abstracts, vocab, embeddings, max_length):
	"""
	Pads abstracts of length less than @max_length with a <NULL> token and
	truncates abstracts of length more than @max_length. Also adds a <NULL>
	token to the vocabulary list, and also vector of zeros to the word embeddings 
	to represent the <NULL> token.
	"""
	# ensure that <NULL> token doesnt yet exist in vocabulary
	assert "<NULL>" not in vocab, "<NULL> token already exists in vocabulary. Use different null token" 
	# initialize lists to store padded abstracts and original length
	new_abstracts = []
	orig_length = []
	# loop over original list of abstracts
	for abstract in abstracts:
		# split abstracts into tokens
		tokens = abstract.split(" ")
		orig_length.append(len(tokens))
		# compute difference in lengths
		diff_len = max_length - len(tokens)
		# if abstract is too short, pad with <NULL> token
		if diff_len > 0:
			new_abstracts.append(abstract + "<NULL> "*diff_len)
		# if sentence is too long, truncate to max_length
		elif diff_len < 0: 
			new_abstracts.append(" ".join(tokens[:max_length]))
		# if correct length, don't modify
		else: 
			new_abstracts.append(abstract)
	# get depth of word embeddings
	depth = embeddings.shape[1]
	# return padded abstracts, and expanded vocabulary and word embeddings
	return(new_abstracts, orig_length, np.append(vocab, "<NULL>"), np.append(embeddings, [[0.]*depth], axis=0))

def test_pad_abstracts(abs_file, embed_file, max_length):
	"""
	Tests the pad_abstracts() function.
	"""
	# Load the abstracts in a list and embeddings in a dictionary
	_ , abstracts = load_abstracts(abs_file)
	vocab, embeddings = load_embeddings_array(embed_file)
	# run the pad_abstracts() function
	abstracts_pad, _ , _ = pad_abstracts(abstracts, max_length, vocab, embeddings)
	# check that length of all abstracts equals max_length
	for i in range(len(abstracts_pad)):
		assert len(abstracts_pad[i].split(" ")) == max_length, "Length of abstract %i is %i, not equal to %i"%(i, len(abstracts_pad[i].split(" ")), max_length)

def vectorize_abstracts(abstracts, vocab):
	"""
	Converts each token in each abstract into an integer that represents
	the token's position in the list of vocabulary.
	"""
	# convert vocab list into dictionary with token as the key and position as the value
	vocab_dict = dict(zip(vocab, list(range(len(vocab)))))
	# get index of <NULL> token
	null_index = vocab_dict["<NULL>"]
	# iterate over all abstracts and replace token with its position in vocabulary
	vectorized_abstracts = []
	for abstract in abstracts:
		vectorized_abstracts.append([vocab_dict.get(token, null_index) for token in abstract.split(" ")])
	
	return np.array(vectorized_abstracts)

def get_minibatches(abstracts, lengths, labels, batch_size, shuffle=True):
	"""
	Returns a generator that iterates over all minibatches of the training set.
	Assumes that @abstracts, @lengths, and @labels are all lists.
	"""
	# check if number of abstracts matches number of labels
	assert len(abstracts) == len(labels)
	# create indices for abstracts and shuffle if specified
	num_abstracts = len(abstracts)
	indices = np.arange(num_abstracts)
	if shuffle: np.random.shuffle(indices)
	# break up indices into slices of @batch_size each, and return generator
	for start_index in np.arange(0, num_abstracts, batch_size):
		yield abstracts[start_index:(start_index+batch_size)], lengths[start_index:(start_index+batch_size)], labels[start_index:(start_index+batch_size)]

def test_get_minibatches(abs_file, topics_file, labs_file, batch_size, shuffle=True):
	"""
	Tests the get_minibatches() function.
	"""
	# Load labels and abstracts as lists
	_ , abstracts = load_abstracts(abs_file)
	_ , labels = load_labels(topics_file, labs_file)
	# run the get_minibatches() function to obtain generator
	minibatches = get_minibatches(abstracts, labels, batch_size, shuffle)
	for minibatch in minibatches:
		assert len(minibatch[0]) == batch_size
		assert len(minibatch[1]) == batch_size

if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--lda-topics", type=str, default="lda_topics_final", help="lda_topics file")
	parser.add_argument("--lda-assignments", type=str, default="lda_assignments_final", help="lda_assignments file")
	parser.add_argument("--abs-dir-tok", type=str, default="data/abstracts_tokenized", help="Directory that stores tokenized abstracts.")
	parser.add_argument("--embeddings", type=str, default="glove/embeddings.txt", help="Path to pre-trained word embeddings.")
	args = parser.parse_args()	

	# test_get_minibatches(args.abs_dir_tok, args.lda_topics, args.lda_assignments, batch_size=1000, shuffle=True)
	test_pad_abstracts(args.abs_dir_tok, args.embeddings, max_length=300)
